{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine translation with transformer .ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rehabreda/NLP/blob/master/machine_translation_with_transformer_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De7AQ9NobV4A",
        "colab_type": "text"
      },
      "source": [
        "# Importing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWQBjCxj8tU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7-IBKKFbMGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import math \n",
        "import re \n",
        "import time "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LswpHy_DaPuO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try :\n",
        "  %tensorflow_version 2.x\n",
        "except:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras import layers    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmY93k6ObcIN",
        "colab_type": "text"
      },
      "source": [
        "# Date Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNRjt-O7bfKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## load data \n",
        "with open('/content/drive/My Drive/translation/europarl-v7.fr-en.en','r',encoding='utf-8') as f :\n",
        "  europarl_en=f.read()\n",
        "\n",
        "with open('/content/drive/My Drive/translation/europarl-v7.fr-en.fr','r',encoding='utf-8') as f :\n",
        "  europarl_fr=f.read()  \n",
        "\n",
        "with open('/content/drive/My Drive/translation/nonbreaking_prefix.en','r',encoding='utf-8') as f :\n",
        "  non_breaking_prefix_en=f.read()    \n",
        "\n",
        "with open('/content/drive/My Drive/translation/nonbreaking_prefix.fr','r',encoding='utf-8') as f :\n",
        "  non_breaking_prefix_fr=f.read()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa5DThxRciTR",
        "colab_type": "text"
      },
      "source": [
        " # cleaning data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEYaga0PcnLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get clean list from non breaking words \n",
        "non_breaking_prefix_en=non_breaking_prefix_en.split('\\n')\n",
        "non_breaking_prefix_en=[' '+ prefix +'.' for prefix in non_breaking_prefix_en]\n",
        "\n",
        "non_breaking_prefix_fr=non_breaking_prefix_fr.split('\\n')\n",
        "non_breaking_prefix_fr=[' '+ prefix +'.' for prefix in non_breaking_prefix_fr]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJswR73xdSjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_en=europarl_en\n",
        "for prefix in non_breaking_prefix_en:\n",
        "  corpus_en=corpus_en.replace(prefix,prefix+'$$$')\n",
        "corpus_en=re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)  \n",
        "corpus_en=re.sub(r\".\\$\\$\\$\",'',corpus_en)\n",
        "corpus_en=re.sub(r\"  +\",' ',corpus_en)\n",
        "corpus_en=corpus_en.split('\\n')\n",
        "\n",
        "\n",
        "corpus_fr=europarl_fr\n",
        "for prefix in non_breaking_prefix_fr:\n",
        "  corpus_fr=corpus_fr.replace(prefix,prefix+'$$$')\n",
        "corpus_fr=re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)  \n",
        "corpus_fr=re.sub(r\".\\$\\$\\$\",'',corpus_fr)\n",
        "corpus_fr=re.sub(r\"  +\",' ',corpus_fr)\n",
        "corpus_fr=corpus_fr.split('\\n')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6ud7fR9dUJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(corpus_en[0])\n",
        "print(corpus_fr[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUlm7WiMi4zh",
        "colab_type": "text"
      },
      "source": [
        "# tokenization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRjXPeKPi8YH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en=tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_en,target_vocab_size=2**13)\n",
        "tokenizer_fr=tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_fr,target_vocab_size=2**13)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1dA3Cb-jazK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_EN=tokenizer_en.vocab_size +2\n",
        "VOCAB_SIZE_FR=tokenizer_fr.vocab_size +2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVF4XjhmnZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE_FR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj8Q6aqDmoxk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs=[[VOCAB_SIZE_EN-2]+tokenizer_en.encode(sentence) +[VOCAB_SIZE_EN-1] for sentence in corpus_en]\n",
        "outputs=[[VOCAB_SIZE_FR-2]+ tokenizer_fr.encode(sentence) +[VOCAB_SIZE_FR-1] for sentence in corpus_fr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3Z1JtDdm6z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJADI480oumd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### remove long sentences \n",
        "MAX_LENGTH =20\n",
        "idx_to_remove=[index for index,sent in enumerate(inputs) if len(sent)>MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]\n",
        "\n",
        "idx_to_remove=[index for index,sent in enumerate(outputs) if len(sent)>MAX_LENGTH]\n",
        "for idx in reversed(idx_to_remove):\n",
        "  del inputs[idx]\n",
        "  del outputs[idx]  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvf84XgUpK3c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## padding \n",
        "inputs=tf.keras.preprocessing.sequence.pad_sequences(inputs,padding='post',value=0,maxlen=MAX_LENGTH)\n",
        "outputs=tf.keras.preprocessing.sequence.pad_sequences(outputs,padding='post',value=0,maxlen=MAX_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43T7gk1npL-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 20000\n",
        "\n",
        "dataset=tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
        "dataset=dataset.cache()\n",
        "dataset=dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t81tTlgbmMxF",
        "colab_type": "text"
      },
      "source": [
        "# model building \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIK8CnZanp_H",
        "colab_type": "text"
      },
      "source": [
        "Positional encoding formulae:\n",
        "\n",
        "PE(pos,2i)=sin(pos/100002i/dmodel) \n",
        "\n",
        "PE(pos,2i+1)=cos(pos/100002i/dmodel)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMjgcZp1mPdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### embedding \n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "  def __init__(self):\n",
        "    super(PositionalEncoding,self).__init__()\n",
        "\n",
        "  def get_angles(self,pos,i,d_model):\n",
        "    angles=1/np.power(10000.0,(2*(i//2))/np.float32(d_model))\n",
        "    return pos * angles \n",
        "\n",
        "  def call(self,inputs):\n",
        "    seq_length=inputs.shape.as_list()[-2]\n",
        "    d_model=inputs.shape.as_list()[-1]\n",
        "    angles=self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "    angles[:,0::2]=np.sin(angles[:,0::2])\n",
        "    angles[:,1::2]=np.cos(angles[:,1::2])\n",
        "    pos_encoding=angles[np.newaxis,...]\n",
        "    return inputs + tf.cast(pos_encoding,tf.float32)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyaWLRp0vNv1",
        "colab_type": "text"
      },
      "source": [
        "# Attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f10tKFF8vPql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "  product=tf.matmul(queries,keys,transpose_b=True)\n",
        "  key_dim=tf.cast(tf.shape(keys)[-1],tf.float32)\n",
        "  scaled_product=product / tf.math.sqrt(key_dim)\n",
        "  if mask is not None:\n",
        "    scaled_product += (mask * -1e9)\n",
        "  attention=tf.matmul(tf.nn.softmax(scaled_product,axis=-1),values)  \n",
        "  return attention\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "al3_XaZBwZSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "  def __init__(self,nb_proj):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.nb_proj=nb_proj\n",
        "  def build(self,input_shape):\n",
        "    self.d_model=input_shape[-1]\n",
        "    assert self.d_model % self.nb_proj==0\n",
        "    self.d_proj=self.d_model // self.nb_proj\n",
        "\n",
        "    self.query_lin=layers.Dense(units=self.d_model)\n",
        "    self.key_lin=layers.Dense(units=self.d_model)\n",
        "    self.value_lin=layers.Dense(units=self.d_model)\n",
        "    self.final_lin=layers.Dense(units=self.d_model)\n",
        "\n",
        "\n",
        "  def split_proj(self,inputs,batch_size):\n",
        "     ## inputs: (batch_size, seq_length, d_model) \n",
        "     shape=(batch_size,-1,self.nb_proj,self.d_proj) \n",
        "     splitted_input=tf.reshape(inputs,shape=shape) ##(batch_size, seq_length, nb_proj,d_proj) \n",
        "     return tf.transpose(splitted_input,perm=[0,2,1,3]) ###(batch_size, nb_proj,seq_length,d_proj) \n",
        "\n",
        "  def call(self,queries, keys, values, mask):\n",
        "    batch_size=queries.shape[0]\n",
        "    ### liner \n",
        "    queries=self.query_lin(queries)\n",
        "    keys=self.key_lin(keys)\n",
        "    values=self.value_lin(values)\n",
        "\n",
        "    queries=self.split_proj(queries,batch_size)\n",
        "    keys=self.split_proj(keys,batch_size)\n",
        "    values=self.split_proj(values,batch_size)\n",
        "    attention=scaled_dot_product_attention(queries, keys, values, mask)\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "    concat_attention=tf.reshape(attention,shape=(batch_size,-1,self.d_model))\n",
        "\n",
        "    output=self.final_lin(concat_attention)\n",
        "    return output\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w-KeXP5LGV0",
        "colab_type": "text"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQQByIpH1YOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "  def __init__(self,FFN_units, nb_proj, dropout_rate):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.FFN_units=FFN_units\n",
        "    self.nb_proj=nb_proj\n",
        "    self.dropout_rate=dropout_rate\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.d_model= input_shape[-1]\n",
        "    ##self attention\n",
        "    self.multi_head_attention=MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1=layers.Dropout( self.dropout_rate) \n",
        "    self.norm_1=layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    ## feed forward\n",
        "    self.dense_1=layers.Dense(units=self.FFN_units,activation='relu')\n",
        "    self.dense_2=layers.Dense(units=self.d_model)\n",
        "    self.dropout_2=layers.Dropout(self.dropout_rate)\n",
        "    self.norm_2=layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self,inputs,mask, training) :\n",
        "    ## self attention \n",
        "    attention=self.multi_head_attention(inputs,inputs,inputs,mask)\n",
        "    attention=self.dropout_1(attention,training)\n",
        "    attention =self.norm_1(attention + inputs)\n",
        "\n",
        "    ## feedforward\n",
        "    outputs=self.dense_1(attention)\n",
        "    outputs=self.dense_2(outputs)\n",
        "    outputs=self.dropout_2(outputs,training)\n",
        "    outputs=self.norm_2(outputs+attention)\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bz2fIHdl8NNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "      super(Encoder, self).__init__(name=name)\n",
        "      self.nb_layers=nb_layers\n",
        "      self.d_model=d_model\n",
        "      self.embedding=layers.Embedding(vocab_size,d_model)\n",
        "      self.pos_encoding=PositionalEncoding()\n",
        "      self.dropout=layers.Dropout(dropout_rate)\n",
        "      self.enc_layers=[EncoderLayer(FFN_units,\n",
        "                                     nb_proj,\n",
        "                                     dropout_rate) for _ in range(nb_layers)]\n",
        "    def call(self, inputs, mask, training):\n",
        "      outputs=self.embedding(inputs)\n",
        "      outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "      outputs=self.pos_encoding(outputs)\n",
        "      outputs=self.dropout(outputs,training)\n",
        "\n",
        "      for i in range(self.nb_layers):\n",
        "        outputs=self.enc_layers[i](outputs,mask, training)\n",
        "      return outputs \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QjU-m1dLJvG",
        "colab_type": "text"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SXQ9mOe-e1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "  def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.FFN_units=FFN_units\n",
        "    self.nb_proj=nb_proj\n",
        "    self.dropout_rate=dropout_rate\n",
        "\n",
        "  def build(self,input_shape):\n",
        "    self.d_model=  input_shape[-1]\n",
        "    ## self multi head attention\n",
        "    self.multi_head_attention_1=MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_1=layers.Dropout(self.dropout_rate)\n",
        "    self.norm1=layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Multi head attention combined with encoder output\n",
        "    self.multi_head_attention_2=MultiHeadAttention(self.nb_proj)\n",
        "    self.dropout_2=layers.Dropout(self.dropout_rate)\n",
        "    self.norm2=layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    ##feed forward\n",
        "    self.dense_1=layers.Dense(units=self.FFN_units,activation='relu')\n",
        "    self.dense_2=layers.Dense(units=self.d_model)\n",
        "    self.dropout_3=layers.Dropout(self.dropout_rate)\n",
        "    self.norm3=layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "    ## self multi head attention\n",
        "    attention= self.multi_head_attention_1(inputs,inputs,inputs,mask_1)\n",
        "    attention=self.dropout_1(attention,training)\n",
        "    attention=self.norm1(inputs+attention)\n",
        "\n",
        "    # Multi head attention combined with encoder output\n",
        "    attention_2=self.multi_head_attention_2(attention,enc_outputs,enc_outputs,mask_2)\n",
        "    attention_2=self.dropout_2(attention_2,training)\n",
        "    attention_2=self.norm2(attention+attention_2)\n",
        "\n",
        "    ##feed forward\n",
        "    outputs=self.dense_1(attention_2)\n",
        "    outputs=self.dense_2(outputs)\n",
        "    outputs=self.dropout_3(outputs,training)\n",
        "    outputs=self.norm3(attention_2 +outputs)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtZkSTNtIQte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "  def __init__(self,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "    super(Decoder, self).__init__(name=name)\n",
        "    self.d_model=d_model\n",
        "    self.nb_layers=nb_layers\n",
        "    self.embedding=layers.Embedding(vocab_size,d_model)\n",
        "    self.pos_encoding=PositionalEncoding()\n",
        "    self.dropout=layers.Dropout(dropout_rate)\n",
        "    self.dec_layers=[DecoderLayer(FFN_units,\n",
        "                                  nb_proj,\n",
        "                                  dropout_rate) for i in range(nb_layers) ]\n",
        "\n",
        "  def call(self, inputs, enc_outputs, mask_1, mask_2, training)  :\n",
        "    outputs=self.embedding(inputs)\n",
        "    outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    outputs=self.pos_encoding(outputs)\n",
        "    outputs=self.dropout(outputs,training)\n",
        "    for i in range(self.nb_layers):\n",
        "      outputs=self.dec_layers[i](outputs,enc_outputs, mask_1, mask_2, training)\n",
        "\n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugQE-MSaLMnG",
        "colab_type": "text"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTy2GDRFLAyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 nb_layers,\n",
        "                 FFN_units,\n",
        "                 nb_proj,\n",
        "                 dropout_rate,\n",
        "                 name=\"transformer\"):\n",
        "        super(Transformer, self).__init__(name=name)\n",
        "        self.encoder=Encoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder=Decoder(nb_layers,\n",
        "                               FFN_units,\n",
        "                               nb_proj,\n",
        "                               dropout_rate,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        self.last_linear=layers.Dense(units=vocab_size_dec,name=\"lin_ouput\")\n",
        "\n",
        "  def create_padding_mask(self, seq):\n",
        "    mask=tf.cast(tf.math.equal(seq,0),tf.float32)\n",
        "    return mask[:,tf.newaxis,tf.newaxis,:]\n",
        "\n",
        "  def create_look_ahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "  def call(self, enc_inputs, dec_inputs, training) :\n",
        "    enc_mask= self.create_padding_mask(enc_inputs)\n",
        "    dec_mask_1 = tf.maximum(\n",
        "            self.create_padding_mask(dec_inputs),\n",
        "            self.create_look_ahead_mask(dec_inputs)\n",
        "        )\n",
        "    dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "    enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "    dec_outputs = self.decoder(dec_inputs,\n",
        "                                   enc_outputs,\n",
        "                                   dec_mask_1,\n",
        "                                   dec_mask_2,\n",
        "                                   training)\n",
        "        \n",
        "    outputs = self.last_linear(dec_outputs)\n",
        "        \n",
        "    return outputs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcgeeUEqTQ1w",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txyqKpK-NqPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Hyper-parameters\n",
        "D_MODEL = 128  # 512\n",
        "NB_LAYERS = 4 # 6\n",
        "FFN_UNITS = 512 # 2048\n",
        "NB_PROJ = 8 # 8\n",
        "DROPOUT_RATE = 0.1 # 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
        "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
        "                          d_model=D_MODEL,\n",
        "                          nb_layers=NB_LAYERS,\n",
        "                          FFN_units=FFN_UNITS,\n",
        "                          nb_proj=NB_PROJ,\n",
        "                          dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NRX-Oe5NshH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                            reduction=\"none\")\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    \n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zRcEikHOlOC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "        \n",
        "        self.d_model = tf.cast(d_model, tf.float32)\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "        \n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "leaning_rate = CustomSchedule(D_MODEL)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M4wVLzKUTGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./drive/My Drive/projects/transformer/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dcWxzEKUWfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "    \n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    \n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "        \n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "        \n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "        \n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
        "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "            \n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
        "                                                        ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhYl9TvPMdob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = \\\n",
        "        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\n",
        "    enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([VOCAB_SIZE_FR-2], axis=0)\n",
        "    \n",
        "    for _ in range(MAX_LENGTH):\n",
        "        predictions = transformer(enc_input, output, False)\n",
        "        \n",
        "        prediction = predictions[:, -1:, :]\n",
        "        \n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
        "        \n",
        "        if predicted_id == VOCAB_SIZE_FR-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        \n",
        "    return tf.squeeze(output, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGquX9GiMepq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "    \n",
        "    predicted_sentence = tokenizer_fr.decode(\n",
        "        [i for i in output if i < VOCAB_SIZE_FR-2]\n",
        "    )\n",
        "    \n",
        "    print(\"Input: {}\".format(sentence))\n",
        "    print(\"Predicted translation: {}\".format(predicted_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUJ2NY6UMkk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"This is a really powerful tool!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lrulNwAUeR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}