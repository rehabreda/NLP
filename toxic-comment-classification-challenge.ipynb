{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from  keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input ,GlobalMaxPool1D\n",
    "from keras.layers import Conv1D, MaxPool1D ,Embedding\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some configuration\n",
    "max_sequence_length=100\n",
    "max_vocab_size=20000\n",
    "embedding_dim=100\n",
    "validation_split=.2\n",
    "batch_size=128\n",
    "epochs=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## load  pre trained vector\n",
    "word2vec={}\n",
    "with open('glove/glove.6B.%sd.txt' % embedding_dim,encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vec=np.asarray(values[1:],dtype='float32')\n",
    "        word2vec[word]=vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##load data\n",
    "train=pd.read_csv('toxic-comment-classification-challenge/train.csv')\n",
    "#train.head()\n",
    "sentences=train['comment_text'].fillna(\"DUMMY_VALUE\").values\n",
    "possible_labels=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "targest=train[possible_labels].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#covert sentences to integer\n",
    "tokenizer=Tokenizer(num_words=max_vocab_size)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences=tokenizer.texts_to_sequences(sentences)\n",
    "#sequences[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## get word2int \n",
    "word2idx= tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## pad sequences\n",
    "data=pad_sequences(sequences,maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###prepare embedding metrix\n",
    "num_words=min(max_vocab_size,len(word2idx)+1)\n",
    "embedding_matrix=np.zeros((num_words,embedding_dim))\n",
    "for word,i in word2idx.items():\n",
    "    if i< max_vocab_size:\n",
    "        embedding_vector=word2vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i]=embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## embedding layer\n",
    "embedding_layer=Embedding(num_words,embedding_dim,weights=[embedding_matrix],input_length=max_sequence_length,trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/10\n",
      "127656/127656 [==============================] - 253s 2ms/step - loss: 0.0830 - acc: 0.9735 - val_loss: 0.0735 - val_acc: 0.9763\n",
      "Epoch 2/10\n",
      "127656/127656 [==============================] - 241s 2ms/step - loss: 0.0672 - acc: 0.9773 - val_loss: 0.0692 - val_acc: 0.9768\n",
      "Epoch 3/10\n",
      "127656/127656 [==============================] - 235s 2ms/step - loss: 0.0629 - acc: 0.9784 - val_loss: 0.0691 - val_acc: 0.9762\n",
      "Epoch 4/10\n",
      "127656/127656 [==============================] - 233s 2ms/step - loss: 0.0598 - acc: 0.9792 - val_loss: 0.0708 - val_acc: 0.9780\n",
      "Epoch 5/10\n",
      "127656/127656 [==============================] - 228s 2ms/step - loss: 0.0575 - acc: 0.9797 - val_loss: 0.0781 - val_acc: 0.9778\n",
      "Epoch 6/10\n",
      "127656/127656 [==============================] - 230s 2ms/step - loss: 0.0552 - acc: 0.9807 - val_loss: 0.0738 - val_acc: 0.9776\n",
      "Epoch 7/10\n",
      "127656/127656 [==============================] - 229s 2ms/step - loss: 0.0540 - acc: 0.9810 - val_loss: 0.0738 - val_acc: 0.9777\n",
      "Epoch 8/10\n",
      "127656/127656 [==============================] - 266s 2ms/step - loss: 0.0527 - acc: 0.9813 - val_loss: 0.0743 - val_acc: 0.9774\n",
      "Epoch 9/10\n",
      "127656/127656 [==============================] - 254s 2ms/step - loss: 0.0514 - acc: 0.9815 - val_loss: 0.0751 - val_acc: 0.9775\n",
      "Epoch 10/10\n",
      "127656/127656 [==============================] - 256s 2ms/step - loss: 0.0503 - acc: 0.9820 - val_loss: 0.0752 - val_acc: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x264db670c18>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##building model \n",
    "input_=Input(shape=(max_sequence_length,))\n",
    "x=embedding_layer(input_)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=MaxPool1D(3)(x)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=MaxPool1D(3)(x)\n",
    "x=Conv1D(128,3,activation='relu')(x)\n",
    "x=GlobalMaxPool1D()(x)\n",
    "x=Dense(128,activation='relu')(x)\n",
    "output=Dense(len(possible_labels),activation='sigmoid')(x)\n",
    "model=Model(input_,output)\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.fit(data,targest,batch_size=batch_size,epochs=epochs,validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
