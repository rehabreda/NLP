{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"Text Generation .ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"PN25h3AKCvxu","colab_type":"code","colab":{}},"source":["### import libraries\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt \n","import tensorflow as tf \n","import string \n","import re \n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense,LSTM ,Embedding\n","from keras.models import Sequential\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","import pickle\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPd2UdCyCvx4","colab_type":"code","colab":{}},"source":["## load data \n","def load_doc(filename):\n","    with open(filename,'r') as f:\n","        text=f.read()\n","    return(text)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWtQfICHCvyA","colab_type":"code","colab":{}},"source":["## clean data\n","def clean(text):\n","    text=text.replace('--',' ')\n","    tokens=text.split()\n","    re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n","    tokens=[re_punc.sub('',w) for w in tokens]\n","    tokens=[w for w in tokens if w.isalpha()]\n","    tokens=[w.lower() for w in tokens ]\n","    return tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WbngwJzQCvyG","colab_type":"code","colab":{}},"source":["## save data \n","def save_doc(lines,filename):\n","    data='\\n'.join(lines)\n","    with open(filename,'w') as f:\n","        f.write(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j314-Qw2CvyK","colab_type":"code","colab":{}},"source":["##load data\n","data=load_doc('republic_clean.txt')\n","data[:200]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8I8A1jioCvyS","colab_type":"code","colab":{}},"source":["#clean data\n","tokens=clean(data)\n","tokens[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rXGooE3KCvyV","colab_type":"code","colab":{}},"source":["print('Tolat Tokens %d'%len(tokens))\n","print('Uniqe Tokens %d' %len(set(tokens)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AKa_h_XlCvyZ","colab_type":"code","colab":{}},"source":["length =50+1  ## 50 for input ,1 for output \n","sequences=[]\n","for i in range(length,len(tokens)):\n","    seq=tokens[i-length :i]\n","    ## convert to line\n","    line=' '.join(seq)\n","    sequences.append(line)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9ZypXcVJCvyd","colab_type":"code","colab":{}},"source":["print(sequences[0])\n","print('Total number of sequences %d' %len(sequences))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRv4nZwlCvyi","colab_type":"code","colab":{}},"source":["## save data\n","save_doc(sequences,'republic_sequences.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ER3A1o4iCvyl","colab_type":"code","colab":{}},"source":["## define model\n","def model(vocab_size,sequence_length):\n","    model=Sequential()\n","    model.add(Embedding(vocab_size,50,input_length=sequence_length))\n","    model.add(LSTM(100,return_sequences=True))\n","    model.add(LSTM(100))\n","    model.add(Dense(100,activation='relu'))\n","    model.add(Dense(vocab_size,activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    model.summary()\n","    plot_model(model,to_file='model.png',show_shapes=True)\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iGMz2yDWCvyo","colab_type":"code","colab":{}},"source":["## tokenizer\n","tokenizer=Tokenizer()\n","tokenizer.fit_on_texts(sequences)\n","encoded_sequences=tokenizer.texts_to_sequences(sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_HY8OUsRCvyq","colab_type":"code","colab":{}},"source":["vocab_size=len(tokenizer.word_index) +1\n","vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3842FcTYCvyt","colab_type":"code","colab":{}},"source":["input_data=np.array(encoded_sequences)\n","x,y=input_data[:,:-1],input_data[:,-1]\n","y=to_categorical(y,num_classes=vocab_size)\n","x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDZ4diUbCvyw","colab_type":"code","colab":{}},"source":["model=model(vocab_size,x.shape[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFpQ2ZzTCvyy","colab_type":"code","colab":{}},"source":["plot_model(model,show_shapes=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUYygQcPCvy1","colab_type":"code","colab":{}},"source":["model.fit(x,y,batch_size=128,epochs=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKzCGJCymKdA","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRzznr5qCvy3","colab_type":"code","colab":{}},"source":["## save model\n","model.save('model.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S79ub9FHCvy8","colab_type":"code","colab":{}},"source":["## save tokenizer\n","with open('tokenizer.pkl','wb') as f:\n","    pickle.dump(tokenizer,f)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dgF0CnZkc1l","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWtK83feL5og","colab_type":"code","colab":{}},"source":["## def generate \n","def generate_text(model,tokenizer,seq_length,seed_text,n_words):\n","  result=[]\n","  in_text=seed_text\n","  for _ in range(n_words):\n","    encoded_text=tokenizer.texts_to_sequences([in_text])[0]\n","    padded_text=pad_sequences([encoded_text],maxlen=seq_length, padding='pre', truncating='pre')\n","    prediction_word=model.predict_classes(padded_text,verbose=0)\n","    output_word=''\n","    for w,i in tokenizer.word_index.items():\n","      if prediction_word[0]== i:\n","        output_word=w\n","        break\n","    in_text +=' '+output_word\n","    result.append(output_word)\n","  return ' '.join(result)      \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EBN5GrU-Ol1C","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","from random import randint\n","in_filename = 'republic_sequences.txt'\n","doc = load_doc(in_filename)\n","lines = doc.split('\\n')\n","seq_length = len(lines[0].split()) - 1\n","model = load_model('model.h5')\n","# load the tokenizer\n","tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))\n","# select a seed text\n","seed_text = lines[randint(0,len(lines))]\n","print(seed_text + '\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xBGI7J99O88W","colab_type":"code","colab":{}},"source":["from keras.preprocessing.sequence import pad_sequences\n","generated = generate_text(model, tokenizer, seq_length, seed_text, 50)\n","print(generated)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oQeLn8Azm08s","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}